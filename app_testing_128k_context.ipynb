{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is heavely based on the code by Greg Kamrdt found here: https://github.com/gkamradt/LLMTest_NeedleInAHaystack\n",
    "\n",
    "Main differences:\n",
    "1. Greg's code uses Langchain to run the prompts of the retrieval and the evaluation, while I use directly the GPT-4 API.\n",
    "2. My test covers a sub-set of Greg's test: This notebook focuses in the area the proved to be weaker in the results in GPT-4. I am excluding the 'good' areas as per Greg's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE GPT 4\n",
    "api_key = \"\" \n",
    "org_key = \"\" \n",
    "\n",
    "apikey_path = \"../apikeys/api_openai.key\"\n",
    "orgKey_path = \"../apikeys/api_openai_org.key\"\n",
    "\n",
    "with open(apikey_path, 'r') as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "with open(orgKey_path, 'r') as file:\n",
    "    org_key = file.read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=api_key, organization=org_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GPT(system_prompt, user_prompt):\n",
    "    result = \"\" \n",
    "    prompt_tokens = 0 \n",
    "    completion_tokens = 0\n",
    "    total_tokens = 0\n",
    "\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            messages=[{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": user_prompt}],\n",
    "            temperature=0,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            stop=None,\n",
    "            model=\"gpt-4-1106-preview\",\n",
    "        )     \n",
    "        #result =  response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        result = response.choices[0].message.content \n",
    "        prompt_tokens = response.usage.prompt_tokens\n",
    "        completion_tokens = response.usage.completion_tokens\n",
    "        total_tokens = response.usage.total_tokens\n",
    "\n",
    "        return result, prompt_tokens, completion_tokens, total_tokens\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred on GPT: {e}\")\n",
    "        return None, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(directory):\n",
    "    context = \"\"\n",
    "    for file in glob.glob(directory):\n",
    "        with open(file, 'r') as f:\n",
    "            context += f.read()\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContext(context_length=0):\n",
    "    txt_files_path = \"LLMTest_NeedleInAHaystack/PaulGrahamEssays\"\n",
    "    context = read_files(f\"{txt_files_path}/*.txt\")\n",
    "    context = context.replace(\"\\n\", \" \") \n",
    "\n",
    "    if context_length == 0:\n",
    "        return context \n",
    "    else: \n",
    "        tokens = enc.encode(context)\n",
    "        \n",
    "        selected_tokens = tokens[:context_length] \n",
    "        selected_tokenized_text = enc.decode(selected_tokens)\n",
    "        context = ''.join(selected_tokenized_text) \n",
    "        return context \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_needle(needle, context, depth_percent, needleRepetition=1): \n",
    "    if len(context) > 120000: \n",
    "        context = context[:(len(context) - 500)] \n",
    "\n",
    "    if depth_percent == 100: \n",
    "        context = context + \" \" + needle \n",
    "    else: \n",
    "        for _ in range(needleRepetition):\n",
    "            # Calculate the insertion position\n",
    "            insert_position = int(len(context) * depth_percent / 100)\n",
    "\n",
    "            # Insert y into x at the calculated position\n",
    "            context = context[:insert_position] + \" \" + needle + \" \" + context[insert_position:]\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context(needle, context_length, depth_percent, needleRepetitions=1): \n",
    "    context = getContext(context_length) \n",
    "    context = insert_needle(needle, context, depth_percent, needleRepetitions) \n",
    "    return context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_exists(results, context_length, depth_percent, version, needleRepetitions):\n",
    "    \"\"\"\n",
    "    Checks to see if a result has already been evaluated or not\n",
    "    \"\"\"\n",
    "    conditions_met = []\n",
    "    for result in results:\n",
    "        context_length_met = result['context_length'] == context_length\n",
    "        depth_percent_met = result['depth_percent'] == depth_percent\n",
    "        version_met = result.get('version', 1) == version\n",
    "        reps_met = result['needle_repetitions'] == needleRepetitions\n",
    "        conditions_met.append(context_length_met and depth_percent_met and version_met and reps_met)\n",
    "        \n",
    "    return any(conditions_met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(response, needle, question_to_ask): \n",
    "    accuracy_criteria = {\n",
    "        \"Accuracy criteria\": \"\"\"\n",
    "        Score 1: The answer is completely unrelated to the reference.\n",
    "        Score 3: The answer has minor relevance but does not align with the reference.\n",
    "        Score 5: The answer has moderate relevance but contains inaccuracies.\n",
    "        Score 7: The answer aligns with the reference but has minor errors or omissions.\n",
    "        Score 10: The answer is completely accurate and aligns perfectly with the reference.      \n",
    "        \"\"\"\n",
    "    } \n",
    "\n",
    "    system_prompt = \"You are a helpful AI bot that answers questions for a user. Read the given [CONTEXT], [INQUIRE] and [FACT], then answer the [QUESTION] using the [ACCURACY_CRITERIA]. Keep your response short and direct.\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        Given the following [CONTEXT], [QUESTION], and [RESPONSE], evaluate the response based on the specified [ACCURACY_CRITERIA] and provide a rating.\n",
    "\n",
    "        [CONTEXT]\n",
    "        {needle}\n",
    "\n",
    "        [QUESTION]\n",
    "        {question_to_ask}\n",
    "\n",
    "        [RESPONSE]\n",
    "        {response}\n",
    "\n",
    "        [ACCURACY_CRITERIA]\n",
    "        {accuracy_criteria}\n",
    "\n",
    "        Based on these criteria, please rate the response. Just give the score.\n",
    "\n",
    "    \"\"\" \n",
    "\n",
    "    score, prompt_tokens, completion_tokens, total_tokens = GPT(system_prompt, user_prompt)\n",
    "\n",
    "    return score, prompt_tokens, completion_tokens, total_tokens\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 60000  70000  80000  90000 100000 110000 120000]\n",
      "[20 30 40 50 60 70 80]\n"
     ]
    }
   ],
   "source": [
    "needle = \"\"\"\n",
    "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n",
    "\"\"\"\n",
    "\n",
    "question_to_ask = \"What is the most fun thing to do in San Francisco?\" \n",
    "\n",
    "# This will produce a list of context lengths for each experiment iteration\n",
    "context_lengths = np.round(np.linspace(60000, 120000, num=7, endpoint=True)).astype(int)\n",
    "\n",
    "# This will product a list of document depths to place your random statement (needle) at.\n",
    "# Suggestion: Try out different distributions (like a sigmoid) to test non-evenly space intervals\n",
    "document_depth_percents = np.round(np.linspace(20, 80, num=7, endpoint=True)).astype(int)\n",
    "\n",
    "print(context_lengths) \n",
    "print(document_depth_percents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code will check to see if a context_length, depth percent and version number have already been checked yet\n",
    "# Change the version # if you would like to run the results multiple times.\n",
    "# If you're just testing, then leave as version=1\n",
    "results_version = 2\n",
    "\n",
    "# Run through each iteration of context_lengths and depths\n",
    "needleRepetitions = 1\n",
    "\n",
    "for context_length in context_lengths:\n",
    "    for depth_percent in document_depth_percents: \n",
    "\n",
    "        try:\n",
    "            with open('results.json', 'r') as f:\n",
    "                results = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            results = []\n",
    "            pass\n",
    "\n",
    "        # Checks to see if you've already checked a length/percent/version.\n",
    "        # This helps if the program stop running and you want to restart later\n",
    "        if result_exists(results, context_length, depth_percent, results_version, needleRepetitions ):\n",
    "            continue\n",
    "\n",
    "        context = generate_context(needle, context_length, depth_percent, needleRepetitions) \n",
    "\n",
    "        system_prompt = \"You are a helpful AI bot that answers questions for a user. Read the given [CONTEXT] and answer the given [QUESTION]. Keep your response short and direct.\"\n",
    "        user_prompt = f\"\"\"\n",
    "            [CONTEXT]\n",
    "            {context}\n",
    "\n",
    "            [QUESTION]\n",
    "            What is the most fun thing to do in San Francisco based on my context? Don't give information outside the document.\n",
    "\n",
    "        \"\"\" \n",
    "\n",
    "        response, prompt_tokens, completion_tokens, total_tokens = GPT(system_prompt, user_prompt)\n",
    "\n",
    "        if total_tokens > 0:\n",
    "\n",
    "            # Compare the reponse to the actual needle you placed\n",
    "            score, prompt_tokens2, completion_tokens2, total_tokens2 = evaluate_response(response, needle, question_to_ask)\n",
    "\n",
    "            if total_tokens2 > 0:\n",
    "\n",
    "                print (f\"Score: {score} - Response: {response}\")\n",
    "\n",
    "                results.append({\n",
    "                    # 'context' : context, # Uncomment this line if you'd like to save the context the model was asked to retrieve from. Warning: This will become very large.\n",
    "                    'context_length' : int(context_length),\n",
    "                    'depth_percent' : int(depth_percent),\n",
    "                    'version' : results_version,\n",
    "                    'needle' : needle,\n",
    "                    'needle_repetitions': int(needleRepetitions),\n",
    "                    'model_response' : response,\n",
    "                    'score' : score,\n",
    "                    'prompt_tokens': int(prompt_tokens),\n",
    "                    'completion_tokens': int(completion_tokens),\n",
    "                    'total_tokens': int(total_tokens)\n",
    "                })\n",
    "\n",
    "                print (f\"#{len(results)} Context: {context_length}, Depth: {depth_percent}, Score: {score}\")\n",
    "\n",
    "                # Save results to a JSON file each run\n",
    "                with open('results.json', 'w') as f:\n",
    "                    json.dump(results, f)\n",
    "\n",
    "            # Optional. Sleep for a bit to stay under the rate limit\n",
    "            # Rate limit is 150K tokens/min so it's set at 120K for some cushion\n",
    "            sleep_time = (context_length / 100000)*60\n",
    "            print (f\"Sleeping: {sleep_time}\\n\")\n",
    "            time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
